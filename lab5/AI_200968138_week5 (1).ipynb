{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1\n",
        "\n",
        "The tutorial on MAB was taken"
      ],
      "metadata": {
        "id": "RCLMU5HEtbBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing tf-agents and importing the necessary packages"
      ],
      "metadata": {
        "id": "T3oNNFfeRg-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tf-agents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeyHBeSst717",
        "outputId": "89bae0ba-d2ef-4927-b11b-0b7549da0bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf-agents\n",
            "  Downloading tf_agents-0.15.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gym<=0.23.0,>=0.17.0\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 KB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (2.2.1)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from tf-agents) (8.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.18.0 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (0.19.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from tf-agents) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents) (0.0.8)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.1.8)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow-probability>=0.18.0->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->gym<=0.23.0,>=0.17.0->tf-agents) (3.15.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697661 sha256=8d3c874552e7e4d7a9e30769e6b79016c8f4546262fd58d844667665c9c3e2ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/2f/ab/68bf956c5dde73c1856d981e54292cf58385fb60bca10b7acd\n",
            "Successfully built gym\n",
            "Installing collected packages: pygame, gym, tf-agents\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.23.0 pygame-2.1.0 tf-agents-0.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import abc\n",
        "\n",
        "from tf_agents.agents import tf_agent\n",
        "from tf_agents.drivers import driver\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.policies import tf_policy\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.trajectories import policy_step\n",
        "\n",
        "nest = tf.nest"
      ],
      "metadata": {
        "id": "cRGv7CHdt2r7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2 Exercise-1"
      ],
      "metadata": {
        "id": "Pfe1xA-6Rsck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BanditPyEnvironment(py_environment.PyEnvironment):\n",
        "\n",
        "  def __init__(self, observation_spec, action_spec):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._action_spec = action_spec\n",
        "    super(BanditPyEnvironment, self).__init__()\n",
        "\n",
        "  # Helper functions.\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _empty_observation(self):\n",
        "    return tf.nest.map_structure(lambda x: np.zeros(x.shape, x.dtype),\n",
        "                                 self.observation_spec())\n",
        "\n",
        "  # These two functions below should not be overridden by subclasses.\n",
        "  def _reset(self):\n",
        "    \"\"\"Returns a time step containing an observation.\"\"\"\n",
        "    return ts.restart(self._observe(), batch_size=self.batch_size)\n",
        "\n",
        "  def _step(self, action):\n",
        "    \"\"\"Returns a time step containing the reward for the action taken.\"\"\"\n",
        "    reward = self._apply_action(action)\n",
        "    return ts.termination(self._observe(), reward)\n",
        "\n",
        "  # These two functions below are to be implemented in subclasses.\n",
        "  @abc.abstractmethod\n",
        "  def _observe(self):\n",
        "    \"\"\"Returns an observation.\"\"\"\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def _apply_action(self, action):\n",
        "    \"\"\"Applies `action` to the Environment and returns the corresponding reward.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "JP5TK2hZtvyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K5ddFyhWhuk"
      },
      "outputs": [],
      "source": [
        "class SimplePyEnvironment(BanditPyEnvironment):\n",
        "\n",
        "  def __init__(self):\n",
        "    action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(1,), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    super(SimplePyEnvironment, self).__init__(observation_spec, action_spec)\n",
        "\n",
        "  def _observe(self):\n",
        "    self._observation = np.random.randint(-2, 3, (1,), dtype='int32')\n",
        "    return self._observation\n",
        "\n",
        "  def _apply_action(self, action):\n",
        "    return action * self._observation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environment = SimplePyEnvironment()\n",
        "observation = environment.reset().observation\n",
        "print(\"observation: %d\" % observation)\n",
        "\n",
        "action = 2\n",
        "\n",
        "net_reward = 0\n",
        "\n",
        "for i in range (0,50):\n",
        "  print(\"\\n\")\n",
        "  print(\"iteration %d\" % i)\n",
        "  print(\"action: %d\" % action)\n",
        "  reward = environment.step(action).reward\n",
        "  net_reward += reward\n",
        "  print(\"reward: %f\" % reward)\n",
        "\n",
        "print(\"Net reward after 50 actions: %d\" % net_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqVREaW7tiI9",
        "outputId": "fe12169e-fe4f-4a69-befa-df463b1423ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation: 1\n",
            "\n",
            "\n",
            "iteration 0\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 1\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 2\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 3\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 4\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 5\n",
            "action: 2\n",
            "reward: -2.000000\n",
            "\n",
            "\n",
            "iteration 6\n",
            "action: 2\n",
            "reward: -2.000000\n",
            "\n",
            "\n",
            "iteration 7\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 8\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 9\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 10\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 11\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 12\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 13\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 14\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 15\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 16\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 17\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 18\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 19\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 20\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 21\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 22\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 23\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 24\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 25\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 26\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 27\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 28\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 29\n",
            "action: 2\n",
            "reward: -2.000000\n",
            "\n",
            "\n",
            "iteration 30\n",
            "action: 2\n",
            "reward: -2.000000\n",
            "\n",
            "\n",
            "iteration 31\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 32\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 33\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 34\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 35\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 36\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 37\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 38\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 39\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 40\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 41\n",
            "action: 2\n",
            "reward: 4.000000\n",
            "\n",
            "\n",
            "iteration 42\n",
            "action: 2\n",
            "reward: -2.000000\n",
            "\n",
            "\n",
            "iteration 43\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 44\n",
            "action: 2\n",
            "reward: 0.000000\n",
            "\n",
            "\n",
            "iteration 45\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 46\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 47\n",
            "action: 2\n",
            "reward: 2.000000\n",
            "\n",
            "\n",
            "iteration 48\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "\n",
            "\n",
            "iteration 49\n",
            "action: 2\n",
            "reward: -4.000000\n",
            "Net reward after 50 actions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore the net reward after 50 action is as above and it changes every time we run the experiment"
      ],
      "metadata": {
        "id": "rCzVaBJ4TzXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SignPolicy(tf_policy.TFPolicy):\n",
        "  def __init__(self):\n",
        "    observation_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(1,), dtype=tf.int32, minimum=-2, maximum=2)\n",
        "    time_step_spec = ts.time_step_spec(observation_spec)\n",
        "\n",
        "    action_spec = tensor_spec.BoundedTensorSpec(\n",
        "        shape=(), dtype=tf.int32, minimum=0, maximum=2)\n",
        "\n",
        "    super(SignPolicy, self).__init__(time_step_spec=time_step_spec,\n",
        "                                     action_spec=action_spec)\n",
        "  def _distribution(self, time_step):\n",
        "    pass\n",
        "\n",
        "  def _variables(self):\n",
        "    return ()\n",
        "\n",
        "  def _action(self, time_step, policy_state, seed):\n",
        "    observation_sign = tf.cast(tf.sign(time_step.observation[0]), dtype=tf.int32)\n",
        "    action = observation_sign + 1\n",
        "    return policy_step.PolicyStep(action, policy_state)"
      ],
      "metadata": {
        "id": "EvGniiJTtm36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_environment = tf_py_environment.TFPyEnvironment(environment)"
      ],
      "metadata": {
        "id": "ztges-uiurYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sign_policy = SignPolicy()\n",
        "\n",
        "current_time_step = tf_environment.reset()\n",
        "print('Observation:')\n",
        "print (current_time_step.observation)\n",
        "action = sign_policy.action(current_time_step).action\n",
        "print('Action:')\n",
        "print (action)\n",
        "reward = tf_environment.step(action).reward\n",
        "print('Reward:')\n",
        "print(reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOqFMJIhuYYj",
        "outputId": "284d077b-13a8-455d-f207-63814e2f2c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Action:\n",
            "tf.Tensor([1], shape=(1,), dtype=int32)\n",
            "Reward:\n",
            "tf.Tensor([[0.]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "step = tf_environment.reset()\n",
        "action = 1\n",
        "next_step = tf_environment.step(action)\n",
        "\n",
        "net_reward = 0\n",
        "\n",
        "for i in range (0,50):\n",
        "  print(\"\\n\")\n",
        "  print(\"iteration %d\" % i)\n",
        "  reward = next_step.reward\n",
        "  next_observation = next_step.observation\n",
        "  print(\"Reward: \")\n",
        "  print(reward)\n",
        "  print(\"Next observation:\")\n",
        "  print(next_observation)\n",
        "\n",
        "print(\"Net reward after 50 actions: %d\" % net_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsY9Qc7FucYL",
        "outputId": "e1ab3bae-0fe7-4a9b-d20e-f0f273fc8645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "iteration 0\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 1\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 2\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 3\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 4\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 5\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 6\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 7\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 8\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 9\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 10\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 11\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 12\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 13\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 14\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 15\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 16\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 17\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 18\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 19\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 20\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 21\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 22\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 23\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 24\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 25\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 26\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 27\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 28\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 29\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 30\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 31\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 32\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 33\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 34\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 35\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 36\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 37\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 38\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 39\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 40\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 41\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 42\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 43\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 44\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 45\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 46\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 47\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 48\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "\n",
            "\n",
            "iteration 49\n",
            "Reward: \n",
            "tf.Tensor([[2.]], shape=(1, 1), dtype=float32)\n",
            "Next observation:\n",
            "tf.Tensor([[0]], shape=(1, 1), dtype=int32)\n",
            "Net reward after 50 actions: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3 Exercise-2"
      ],
      "metadata": {
        "id": "KzEWeXnCvJ7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)"
      ],
      "metadata": {
        "id": "SnVdNYFB8ux0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RewardEnv(py_environment.PyEnvironment):\n",
        "  def __init__(self, reward_sign):\n",
        "    self._observation_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=-5, maximum=5, name='observation')\n",
        "    self._action_spec = array_spec.BoundedArraySpec(\n",
        "        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action')\n",
        "    self._episode_ended = False\n",
        "    self._observation = None\n",
        "    self._reward = None\n",
        "    self._reward_sign = reward_sign\n",
        "\n",
        "  def action_spec(self):\n",
        "    return self._action_spec\n",
        "\n",
        "  def observation_spec(self):\n",
        "    return self._observation_spec\n",
        "\n",
        "  def _reset(self):\n",
        "    self._episode_ended = False\n",
        "    self._observation = np.random.randint(low=-5, high=6)\n",
        "    self._reward = 0\n",
        "    return ts.restart(np.array(self._observation, dtype=np.int32))\n",
        "\n",
        "  def _step(self, action):\n",
        "    if self._episode_ended:\n",
        "      return self.reset()\n",
        "\n",
        "    if self._reward_sign == 'original':\n",
        "      self._reward = self._observation * action\n",
        "    else:\n",
        "      self._reward = -self._observation * action\n",
        "\n",
        "    self._episode_ended = True\n",
        "    return ts.termination(np.array(self._observation, dtype=np.int32), reward=self._reward)\n"
      ],
      "metadata": {
        "id": "XW5foVA2dqCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b)"
      ],
      "metadata": {
        "id": "Ql6aGiy98zMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "  def __init__(self):\n",
        "    self._state = 'unknown'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    if self._state == 'unknown':\n",
        "      if observation >= 0:\n",
        "        self._state = 'original'\n",
        "        return 2\n",
        "      else:\n",
        "        self._state = 'flipped'\n",
        "        return 0\n",
        "    elif self._state == 'original':\n",
        "      return 2\n",
        "    else:\n",
        "      return 0"
      ],
      "metadata": {
        "id": "ohzoGg1d8wRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c)"
      ],
      "metadata": {
        "id": "WZH73Ypk83i2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self):\n",
        "    self._policy = Policy()\n",
        "\n",
        "  def update_policy(self, reward_sign):\n",
        "    if reward_sign == 'original':\n",
        "      self._policy._state = 'original'\n",
        "    else:\n",
        "      self._policy._state = 'flipped'\n",
        "\n",
        "  def get_action(self, observation):\n",
        "    return self._policy.get_action(observation)"
      ],
      "metadata": {
        "id": "XvIvWLmA82Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output from each case"
      ],
      "metadata": {
        "id": "--tP4LPI871G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original Environment (reward = observation * action)"
      ],
      "metadata": {
        "id": "CTM4Tq-W9B7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='original')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('original')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damd7Rql865f",
        "outputId": "98f53a6e-d00a-4469-b879-e8239ec4df07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward:  28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The total reward should be positive since the optimal policy will result in positive rewards for positive observations and negative rewards for negative observations."
      ],
      "metadata": {
        "id": "IEAjuZS-9Vo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flipped Environment (reward = -observation * action)"
      ],
      "metadata": {
        "id": "Ez0XHRQ69L7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reward_env = RewardEnv(reward_sign='flipped')\n",
        "agent = Agent()\n",
        "\n",
        "total_reward = 0\n",
        "for i in range(50):\n",
        "    observation = reward_env.reset().observation\n",
        "    agent.update_policy('flipped')\n",
        "    action = agent.get_action(observation)\n",
        "    time_step = reward_env.step(action)\n",
        "    total_reward += time_step.reward\n",
        "print(\"Total reward: \", total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSMGGzMx9EQP",
        "outputId": "5481bc8c-a10c-498d-db76-2713fafb0b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This defines an environment where the reward is always 0, regardless of the action and observation. Therefore, the total reward in this environment should always be 0."
      ],
      "metadata": {
        "id": "iHleQus29apT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxuTRxh49Ou-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}