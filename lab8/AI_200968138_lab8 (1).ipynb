{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 8 \n",
        "- Anant Sabharwal\n",
        "- 200968138"
      ],
      "metadata": {
        "id": "O6Omd7_ShVsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary libraries"
      ],
      "metadata": {
        "id": "it9x4rv-hjNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i8rwzgxBhQFe"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baUB-lZvhsvy",
        "outputId": "d480908b-f45e-4b97-cf05-81448fd30acb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration Function"
      ],
      "metadata": {
        "id": "V0FGmnRAiMNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(policy, env, discount_factor, theta, max_iterations):\n",
        "    '''Function to implement policy iteration\n",
        "    '''\n",
        "\n",
        "    num_s = env.observation_space.n\n",
        "    num_act = env.action_space.n\n",
        "    val_func = np.zeros(num_s)\n",
        "    for i in range(max_iterations):\n",
        "        while True:\n",
        "            d = 0\n",
        "            for s in range(num_s):\n",
        "                old_val = val_func[s]\n",
        "                act_p = policy[s]\n",
        "                act_val = np.zeros(num_act)\n",
        "                for a in range(num_act):\n",
        "                    for p, next, reward, done in env.P[s][a]:\n",
        "                        act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "                val_func[s] = np.sum(act_p * act_val)\n",
        "                d = max(d, np.abs(old_val - val_func[s]))\n",
        "            if d < theta:\n",
        "                break\n",
        "        policy_stable = True\n",
        "        for s in range(num_s):\n",
        "            old_act = np.argmax(policy[s])\n",
        "            act_val = np.zeros(num_act)\n",
        "            for a in range(num_act):\n",
        "                for p, next, reward, done in env.P[s][a]:\n",
        "                    act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "            best_act = np.argmax(act_val)\n",
        "            if old_act != best_act:\n",
        "                policy_stable = False\n",
        "            policy[s] = np.eye(num_act)[best_act]\n",
        "        if policy_stable:\n",
        "            break  \n",
        "    return policy\n"
      ],
      "metadata": {
        "id": "Grh9Kge2hvTl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Iteration Function"
      ],
      "metadata": {
        "id": "XFa1E414l732"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(env, discount_factor, theta, max_iterations):\n",
        "    '''Function to implement value iteration\n",
        "    '''\n",
        "    num_s = env.observation_space.n\n",
        "    num_act = env.action_space.n\n",
        "    val_func = np.zeros(num_s)\n",
        "    for i in range(max_iterations):\n",
        "        d = 0\n",
        "        for s in range(num_s):\n",
        "            old_val = val_func[s]\n",
        "            act_val = np.zeros(num_act)\n",
        "            for a in range(num_act):\n",
        "                for p, next, reward, done in env.P[s][a]:\n",
        "                    act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "            val_func[s] = np.max(act_val)\n",
        "            d = max(d, np.abs(old_val - val_func[s]))\n",
        "        if d < theta:\n",
        "            break\n",
        "    policy = np.zeros((num_s, num_act))\n",
        "    for s in range(num_s):\n",
        "        act_val = np.zeros(num_act)\n",
        "\n",
        "        for a in range(num_act):\n",
        "            for p, next, reward, done in env.P[s][a]:\n",
        "                act_val[a] += p*(reward + discount_factor * val_func[next])\n",
        "\n",
        "        best_act = np.argmax(act_val)\n",
        "        policy[s][best_act] = 1.0    \n",
        "    return policy"
      ],
      "metadata": {
        "id": "ILjHDy7Ulws3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Based on the number of episodes"
      ],
      "metadata": {
        "id": "2yCxwcQDmJML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def episodes(policy, env, episodes):\n",
        "    '''Function to eun the episodes\n",
        "    '''\n",
        "    tot_reward = 0\n",
        "    wins = 0\n",
        "    for i in range(episodes):\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "        flag = False\n",
        "        while not flag:\n",
        "            act = np.random.choice(env.action_space.n, p=policy[state])\n",
        "            next, reward, flag, _ = env.step(act)\n",
        "            ep_reward += reward\n",
        "            state = next\n",
        "        tot_reward += ep_reward\n",
        "        if ep_reward == 1:\n",
        "            wins += 1\n",
        "    avg_reward = tot_reward/episodes\n",
        "    return wins, avg_reward"
      ],
      "metadata": {
        "id": "reRFswSRmFyf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters\n",
        "n_states = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "discount_factor = 0.9\n",
        "theta = 1e-8\n",
        "max_iterations = 1000\n",
        "n_episode = 1000"
      ],
      "metadata": {
        "id": "FDw1vVLEmSKC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running policy iteration\n",
        "policy = np.ones([n_states, n_actions]) / n_actions\n",
        "opt_policy1 = policy_iteration(policy, env, discount_factor, theta, max_iterations)\n",
        "wins1, avg_reward1 = episodes(opt_policy1, env, n_episode)"
      ],
      "metadata": {
        "id": "IlK14Zp8mUs4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running value iteration\n",
        "opt_policy2 = value_iteration(env, discount_factor, theta, max_iterations)\n",
        "wins2, avg_reward2 = episodes(opt_policy2, env, n_episode)"
      ],
      "metadata": {
        "id": "s7NuXEk5mXGf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Policy Iteration:\", f\"Number of wins = {wins1}\", f\"Average Return = {avg_reward1}\",\"\", sep = '\\n')\n",
        "print(\"Value Iteration:\", f\"Number of wins = {wins2}\", f\"Average Return = {avg_reward2}\",\"\", sep = '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s2x0dHlmaId",
        "outputId": "d93a40c4-261a-4a6d-972a-29efc3bfacdf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration:\n",
            "Number of wins = 722\n",
            "Average Return = 0.722\n",
            "\n",
            "Value Iteration:\n",
            "Number of wins = 710\n",
            "Average Return = 0.71\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that both methods, (a) Policy Iteration and (b) Value Iteration successfully converged to the optimal policy for the OpenAI FrozenLake-v1 environment, as seen by the high number of wins for each approach. \n",
        "\n",
        "However, we observe that for Policy Iteration, it has a higher number of wins and hence higher average reward. \n",
        "\n",
        "This implies that policy iteration may have started to converge to the optimal policy faster than Value iteration. \n",
        "\n",
        "This is applicable only when the discount factor is 0.9 and the results may change for different values of dicount factor.\n",
        "\n",
        "Thus, from the results from this notebook, (at a discount factor of 0.9, max iterations of 1000) value iteration is the better choice to solve the FrozenLake-v1 enviorment. "
      ],
      "metadata": {
        "id": "AwqvW5OvmmUI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_XYxvRhmdu_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}